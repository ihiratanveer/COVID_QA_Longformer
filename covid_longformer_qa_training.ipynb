{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yIyichji6Xj"
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u18dSilGkKUv"
   },
   "outputs": [],
   "source": [
    "# !python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mpULxQOiylx"
   },
   "outputs": [],
   "source": [
    "# Original file is located at\n",
    "#     https://colab.research.google.com/drive/1UxKWCRsrgot1xmCKCGwC9RBlalkamDhT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoDNHhDCiylx"
   },
   "source": [
    "Longformer for Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHC7siN_kVXB"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi\n",
    "!git clone https://github.com/huggingface/transformers.git\n",
    "# !pip install -U ./transformers\n",
    "# !pip uninstall transformers\n",
    "!pip install transformers==2.11.0\n",
    "!pip install pytorch\n",
    "!pip install git+https://github.com/huggingface/nlp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wm59tRrJiO1o"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgte6YAFi9qc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6KQsBjLiyl0"
   },
   "source": [
    "\n",
    "the Longformer model was presented in [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan. As the paper explains it  `Longformer` is a BERT-like model fo r long documents. Training longformer for QA is similar to how you train BERT for QA. But there few things to keep in mind when using longformer for QA task. Longformer uses sliding-window local attention which scales linearly with sequence length. This is what allows longformer to handle longer sequences. For more details on how the sliding window attention works, please refer to the paper. Along with local attention longformer also allows you to use global attention for certain tokens. For QA task, all question tokens should have global attention. The attention is configured using the `attention_mask` paramter of the `forward` method of `LongformerForQuestionAnswering`. Mask values are selected in [0, 1, 2]: 0 for no attention (padding tokens), 1 for local attention (a sliding window attention), 2 for global attention (tokens that attend to all other tokens, and all other tokens attend to them). As stated above all question tokens should be given gloabl attention. The `LongformerForQuestionAnswering` model handles this automatically for you. To allow it to do that\n",
    "1. The input sequence must have three sep tokens, i.e the sequence should be encoded like this `<s> question</s></s> context</s>`. If you encode the question and answer as a input pair, then the tokenizer already takes care of that, you shouldn't worry about it.\n",
    "2. input_ids should always be a batch of examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEVKyIYDiymE"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "# import accelerate\n",
    "# import torch\n",
    "\n",
    "transformers_version = transformers.__version__\n",
    "# accelerate_version = accelerate.__version__\n",
    "# pytorch_version = torch.__version__\n",
    "\n",
    "print(\"Transformers version:\", transformers_version)\n",
    "# print(\"Accelerate version:\", accelerate_version)\n",
    "# print(\"PyTorch version:\", pytorch_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fFAvTwpiymF"
   },
   "source": [
    "\n",
    "## Load and process data\n",
    "Here we are using the awesome new nlp library to load and process the dataset.\n",
    "Also we will use Transformers's fast tokenizers alignement methods to get position of answer spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FklhZnPCiymF"
   },
   "outputs": [],
   "source": [
    "!pip install nlp\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFC5KfH6iymI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import nlp\n",
    "from transformers import LongformerTokenizerFast\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGG-5MAGiymU"
   },
   "outputs": [],
   "source": [
    "model_name = \"valhalla/longformer-base-4096-finetuned-squadv1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggLEZsjptBaV"
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_name, num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExNE93JOjLBW"
   },
   "outputs": [],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkpWaQEIiymU"
   },
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qc1DqSzdiymV"
   },
   "outputs": [],
   "source": [
    "def get_correct_alignement(context, answers):\n",
    "    \"\"\" Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here. \"\"\"\n",
    "    # print(answers)\n",
    "    gold_text = answers['text'][0]\n",
    "    # print(gold_text)\n",
    "    start_idx = answers['answer_start'][0]\n",
    "    end_idx = start_idx + len(gold_text)\n",
    "    return start_idx, end_idx      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1C-H9GyiymV"
   },
   "source": [
    "Tokenize our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "qSQj4SULiymV"
   },
   "outputs": [],
   "source": [
    "def convert_to_features(example):\n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    input_pairs = [example['question'], example['context']]\n",
    "    encodings = tokenizer.encode_plus(input_pairs, pad_to_max_length=True, max_length=512)\n",
    "    context_encodings = tokenizer.encode_plus(example['context'])\n",
    "\n",
    "\n",
    "    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
    "    # this will give us the position of answer span in the context text\n",
    "    start_idx, end_idx = get_correct_alignement(example['context'], example['answers'])\n",
    "    # print(\"print(start_idx)\", type(start_idx))\n",
    "\n",
    "    start_positions_context = context_encodings.char_to_token(start_idx)\n",
    "    end_positions_context = context_encodings.char_to_token(end_idx-1)\n",
    "    try:\n",
    "      start_positions_context = int(start_positions_context)\n",
    "    except TypeError:\n",
    "      start_positions_context = 0\n",
    "    try:\n",
    "      end_positions_context = int(end_positions_context)\n",
    "    except TypeError:\n",
    "      end_positions_context = 0\n",
    "    # print(start_positions_context)\n",
    "    # print(type(end_positions_context))\n",
    "\n",
    "    # here we will compute the start and end position of the answer in the whole example\n",
    "    # as the example is encoded like this <s> question</s></s> context</s>\n",
    "    # and we know the postion of the answer in the context\n",
    "    # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)\n",
    "    # this will give us the position of the answer span in whole example\n",
    "    sep_idx = encodings['input_ids'].index(tokenizer.sep_token_id)\n",
    "    # print(type(sep_idx))\n",
    "    start_positions = start_positions_context + sep_idx + 1\n",
    "    end_positions = end_positions_context + sep_idx + 1\n",
    "    if end_positions > 512:\n",
    "      start_positions, end_positions = 0, 0\n",
    "    encodings.update({'start_positions': start_positions,\n",
    "                      'end_positions': end_positions,\n",
    "                      'attention_mask': encodings['attention_mask']})\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CxVNqH8BiymW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/beyond-data/anaconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: datasets in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (2.13.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: xxhash in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: packaging in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: aiohttp in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: multiprocess in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: importlib-metadata in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (6.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pandas in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from aiohttp->datasets) (4.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/beyond-data/anaconda3/envs/testqa/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "uDQphdi8tI4_"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRNHvThQiymW"
   },
   "source": [
    "load train and validation split of squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XWiDzm_ViymX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset covid_qa_deepset (/home/beyond-data/.cache/huggingface/datasets/covid_qa_deepset/covid_qa_deepset/1.0.0/fb886523842e312176f92ec8e01e77a08fa15a694f5741af6fc42796ee9c8c46)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dict:  Dataset({\n",
      "    features: ['document_id', 'context', 'question', 'is_impossible', 'id', 'answers'],\n",
      "    num_rows: 1817\n",
      "})\n",
      "valid_dict:  Dataset({\n",
      "    features: ['document_id', 'context', 'question', 'is_impossible', 'id', 'answers'],\n",
      "    num_rows: 202\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_dict  = (load_dataset('covid_qa_deepset', split='train').train_test_split(test_size=0.1))\n",
    "# valid_dataset = nlp.load_dataset('squad', split=nlp.Split.VALIDATION)\n",
    "train_dataset = dataset_dict['train']\n",
    "valid_dataset = dataset_dict['test']\n",
    "# print(\"dataset_dict: \",dataset_dict)\n",
    "print(\"train_dict: \",train_dataset)\n",
    "print(\"valid_dict: \",valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zjCwOheHiymY"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/202 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(convert_to_features)\n",
    "valid_dataset = valid_dataset.map(convert_to_features, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QktgSpvbiymZ"
   },
   "source": [
    "set the tensor type and the columns which the dataset should return\n",
    "columns = ['answers', 'context', 'document_id', 'id', 'is_impossible', 'question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Mry7Dm4LiymZ"
   },
   "outputs": [],
   "source": [
    "columns = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "3qSI-qYOiyma"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1817, 202)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VahwZtMXiyma"
   },
   "source": [
    "cach the dataset, so we can load it directly for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "3uu2AkJ7iyma"
   },
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'train_data.pt')\n",
    "torch.save(valid_dataset, 'valid_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HJrxLP1iyma"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "FZiId_0tiyma"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import LongformerForQuestionAnswering, LongformerTokenizerFast, EvalPrediction\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    DataCollator,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class DummyDataCollator():\n",
    "    def collate_batch(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Take a list of samples from a Dataset and collate them into a batch.\n",
    "        Returns:\n",
    "            A dictionary of tensors\n",
    "        \"\"\"\n",
    "        input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "        start_positions = torch.stack([example['start_positions'] for example in batch])\n",
    "        end_positions = torch.stack([example['end_positions'] for example in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'start_positions': start_positions,\n",
    "            'end_positions': end_positions,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "    train_file_path: Optional[str] = field(\n",
    "        default='train_data.pt',\n",
    "        metadata={\"help\": \"Path for cached train dataset\"},\n",
    "    )\n",
    "    valid_file_path: Optional[str] = field(\n",
    "        default='valid_data.pt',\n",
    "        metadata={\"help\": \"Path for cached valid dataset\"},\n",
    "    )\n",
    "    max_len: Optional[int] = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Max input length for the source text\"},\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "\n",
    "    # we will load the arguments from a json file,\n",
    "    # make sure you save the arguments in at ./args.json\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('args.json'))\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        # training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "\n",
    "    tokenizer = LongformerTokenizerFast.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    model = LongformerForQuestionAnswering.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir\n",
    "    )\n",
    "\n",
    "    # Get datasets\n",
    "    print('loading data')\n",
    "    train_dataset  = torch.load(data_args.train_file_path)\n",
    "    valid_dataset = torch.load(data_args.valid_file_path)\n",
    "    print('loading done')\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=DummyDataCollator(),\n",
    "        prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        trainer.train(\n",
    "            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "        )\n",
    "        trainer.save_model()\n",
    "        # For convenience, we also re-save the tokenizer to the same directory,\n",
    "        # so that you can share your model easily on huggingface.co/models =)\n",
    "        if trainer.is_world_master():\n",
    "            tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if training_args.do_eval and training_args.local_rank in [-1, 0]:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        eval_output = trainer.evaluate()\n",
    "\n",
    "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(eval_output.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(eval_output[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(eval_output[key])))\n",
    "\n",
    "        results.update(eval_output)\n",
    "\n",
    "    return results, model\n",
    "\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30qddN9Wiymc"
   },
   "source": [
    "\n",
    "## Train\n",
    "\n",
    "import json\n",
    "et's write the arguments in a dict and store in a json file. The above code will load this file and parse the arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5zakDUHtvjS"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnkBfJ9Viymc"
   },
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "  # \"n_gpu\": 1,\n",
    "  \"model_name_or_path\": \"valhalla/longformer-base-4096-finetuned-squadv1\",\n",
    "  \"max_len\": 512 ,\n",
    "  \"output_dir\": './models',\n",
    "  \"overwrite_output_dir\": True,\n",
    "  \"per_gpu_train_batch_size\": 8,\n",
    "  \"per_gpu_eval_batch_size\": 8,\n",
    "  \"gradient_accumulation_steps\": 16,\n",
    "  \"learning_rate\": 1e-4,\n",
    "  \"num_train_epochs\": 3,\n",
    "  \"do_train\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lbuIcKOiymc"
   },
   "outputs": [],
   "source": [
    "with open('args.json', 'w') as f:\n",
    "  json.dump(args_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q20Q3T4RlJPX"
   },
   "source": [
    "Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJH3_GiMlE_L"
   },
   "outputs": [],
   "source": [
    "results, model = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f41-11fLiymc"
   },
   "source": [
    "\n",
    "# Eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M_62ulziymv"
   },
   "source": [
    " SQuAD evaluation script. Modifed slightly for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "hj5kyTnbiymv"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "wPlw4dQIiymv"
   },
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qAHuW1Phiymw"
   },
   "outputs": [],
   "source": [
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "GzgW9XP-iymw"
   },
   "outputs": [],
   "source": [
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "zr5d8qSRiymw"
   },
   "outputs": [],
   "source": [
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "_qVM4gPDiymw"
   },
   "outputs": [],
   "source": [
    "def evaluate(gold_answers, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "    for ground_truths, prediction in zip(gold_answers, predictions):\n",
    "      total += 1\n",
    "      exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "      f1 += metric_max_over_ground_truths(\n",
    "          f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Mk0jQzbJiymx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizerFast, LongformerForQuestionAnswering\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-d3vMJOhiymx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerForQuestionAnswering(\n",
       "  (longformer): LongformerModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = LongformerTokenizerFast.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "# model = LongformerForQuestionAnswering.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "# model = model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "kk2vzDzliymx"
   },
   "outputs": [],
   "source": [
    "valid_dataset = torch.load('valid_data.pt')\n",
    "dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "APocAL8Xiymx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97acc28e2e484836a5bd07ae06c7c022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers = []\n",
    "with torch.no_grad():\n",
    "  for batch in tqdm(dataloader):\n",
    "    start_scores, end_scores = model(input_ids=batch['input_ids'].cuda(),\n",
    "                                  attention_mask=batch['attention_mask'].cuda())\n",
    "    for i in range(start_scores.shape[0]):\n",
    "      all_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][i])\n",
    "      answer = ' '.join(all_tokens[torch.argmax(start_scores[i]) : torch.argmax(end_scores[i])+1])\n",
    "      ans_ids = tokenizer.convert_tokens_to_ids(answer.split())\n",
    "      answer = tokenizer.decode(ans_ids)\n",
    "      answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' threat to populations', ' can interact with pathogens including HIV-1', ' Nucleolar Protein Trafficking in Response to HIV-1 Tat: Rewiring the Nucleolus\\n\\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3499507/\\n\\nSHA: efa871aeaf22cbd0ce30e8bd1cb3d1afff2a98f9\\n\\nAuthors: Jarboui, Mohamed Ali; Bidoia, Carlo; Woods, Elena; Roe, Barbara; Wynne, Kieran; Elia, Giuliano; Hall, William W.; Gautier, Virginie W.\\nDate: 2012-11-15\\nDOI: 10.1371/journal.pone.0048702\\nLicense: cc-by\\n\\nAbstract: The trans-activator Tat protein is a viral regulatory protein essential for HIV-1 replication. Tat trafficks to the nucleoplasm and the nucleolus. The nucleolus, a highly dynamic and structured membrane-less sub-nuclear compartment, is the site of rRNA and ribosome biogenesis and is involved in numerous cellular functions including transcriptional regulation, cell cycle control and viral infection. Importantly, transient nucleolar trafficking of both Tat and HIV-1 viral transcripts are critical in HIV-1 replication, however, the role(s) of the nucleolus in HIV-1 replication remains unclear. To better understand how the interaction of Tat with the nucleolar machinery contributes to HIV-1 pathogenesis, we investigated the quantitative changes in the composition of the nucleolar proteome of Jurkat T-cells', 'Zixing Huang and Shuang Zhao', ' Potentially Emerging Epidemic', ' 72 hours', ' high risk of nosocomial transmission, (4) unpredictability of size impacted', ' a large wet market', ' Chikungunya', ' high morbidity and even mortality worldwide', ' COVID-19 surveillance', ' high morbidity and even mortality', ' f7c3160bef4169d29e2a8bdd79dd6e9056d4774c\\n\\nAuthors: Thiboutot, Michelle M.; Kannan, Senthil; Kawalekar, Omkar U.; Shedlock, Devon J.; Khan, Amir S.; Sarangan, Gopalsamy; Srikanth, Padma; Weiner, David B.; Muthumani, Karuppiah\\nDate: 2010-04-27\\nDOI: 10.1371/journal.pntd.0000623\\nLicense: cc-by\\n\\nAbstract: Chikungunya virus is a mosquito-borne emerging pathogen that has a major health impact in humans and causes fever disease, headache, rash, nausea, vomiting, myalgia, and arthralgia. Indigenous to tropical Africa, recent large outbreaks have been reported in parts of South East Asia and several of its neighboring islands in 2005–07 and in Europe in 2007. Furthermore, positive cases have been confirmed in the United States in travelers returning from known outbreak areas. Currently, there is no vaccine or antiviral treatment. With the threat of an emerging global pandemic, the peculiar problems associated with the more immediate and seasonal epidemics warrant the development of an effective vaccine. In this review, we summarize the evidence supporting these concepts.\\n\\nText: Chikungunya virus (CHIKV), a mosquito-borne pathogen listed by National Institute of Allergy and Infectious Diseases (NIAID) as a Category C Priority Pathogen that causes Chikungunya fever (CHIKF), has been spreading throughout Asia, Africa, and parts of Europe in recent times [1, 2, 3]. CHIKV is an arthropod-borne virus (arbovirus) and is transmitted to humans primarily by Aedes aegypti, the infamous yellow fever propagator [4, 5', ' f13c88733ea45be9e923a282dfd42f8c277c187c', ' Spanish Pyrenees', ' global economies and reputations', ' serum viral RNA', ' surface antigens', ' epigenetic modifications', ' S-palmitoylated', ' large population sizes and fast generation times', ' randomly selected Soest citizens', ' Virus-Vectored Influenza Virus', '<s>', ' Protective Effects', '30', ' 5d254ed178c092d3639ce70ae9653593acc471f9', ' 2019 novel human-pathogenic coronavirus', ' Katanin p60 subunit A-like 1', ' the prevalence of different respiratory viral infections in causing exacerbations in chronic airway inflammatory diseases', ' Control measures and changes in population behaviour', ' to elicit neutralizing antibodies and to protect against homologous and heterologous HPAI H5N1 strain challenge', ' 764 (60.8%)', ' First cases of coronavirus disease 2019 (COVID-19) in the WHO European Region, 24 January to 21 February 2020\\n\\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC7068164/\\n\\nSHA: ce358c18aac69fc83c7b2e9a7dca4a43b0f60e2e', ' Multivalent HA DNA Vaccination Protects against Highly Pathogenic H5N1 Avian Influenza Infection in Chickens and Mice', ' efe13a8d42b60ef9f7387ea539a1b2eeb5f80101', ' ee6d70a53e3262cea6f85bd8b226f6b4c8b5f64b', '<s>', ' single-stranded RNA virus', ' 764', ' f02d0c1e8b0109648e578662dc250abe349a033c\\n\\nAuthors: Chen, I-Yin; Moriyama, Miyu; Chang, Ming-Fu; Ichinohe, Takeshi\\nDate: 2019-01-29\\nDOI: 10.3389/fmicb.2019.00050\\nLicense: cc-by\\n\\nAbstract: Nod-like receptor family, pyrin domain-containing 3 (NLRP3) regulates the secretion of proinflammatory cytokines interleukin 1 beta (IL-1β) and IL-18. We previously showed that influenza virus M2 or encephalomyocarditis virus (EMCV) 2B proteins stimulate IL-1β secretion following activation of the NLRP3 inflammasome. However, the mechanism by which severe acute respiratory syndrome coronavirus (SARS-CoV) activates the NLRP3 inflammasome remains unknown', ' Glycyrrhizin', ' hepcidin', ' 1.64', ' Bats host virulent zoonotic viruses without experiencing disease', ' increases airway inflammation which aggravates disease symptoms', ' Beijing Genomic Institute', '<s>', ' No credible evidence supporting claims of the laboratory engineering', ' neglected human pathogen of clinical significance. Concerns about human-pathogenic mammarenaviruses are exacerbated by of the lack of licensed vaccines, and current anti-mammarenavirus therapy is limited to off-label use of ribavirin that is only partially effective. Detailed understanding of virus/host-cell interactions may facilitate the development of novel anti-mammarenavirus strategies by targeting components of the host-cell machinery that are required for efficient virus multiplication. Here we document the generation of a recombinant LCMV encoding a nucleoprotein (NP) containing an affinity tag (rLCMV/Strep-NP) and its use to capture the NP-interactome in infected cells. Our proteomic approach combined with genetics and pharmacological validation assays identified ATPase Na(+)/K(+) transporting subunit alpha 1 (ATP1A1) and prohibitin (PHB) as pro-viral factors. Cell-based assays revealed that ATP1A1 and PHB are involved in different steps of the virus life cycle', ' Staphylococcus aureus', ' The immune response against some viral pathogens, in particular those causing chronic infections, is often ineffective notwithstanding a robust humoral neutralizing response', 'the problems...are now so deeply ingrained', ' Wuhan, China', ' use in humans', ' virus infectivity assays on bat cell lines expressing induced and constitutive immune phenotypes', '30.4%)', '', ' Many questions about its\\norigins, its unusual epidemiologic features, and the basis of\\nits pathogenicity remain unanswered', ' vaccine carrier', '', ' Constant evolution of circulating influenza virus strains and the emergence of new strains diminishes the effectiveness of annual vaccines that rely on a match with circulating influenza strains. Thus, there is a continued need for new, efficacious vaccines conferring cross-clade protection to avoid the need for biannual reformulation of seasonal influenza vaccines. Recombinant virus-vectored vaccines are an appealing alternative to classical inactivated vaccines because virus vectors enable native expression of influenza antigens, even from virulent influenza viruses, while expressed in the context of the vector that can improve immunogenicity', ' Novel Coronavirus\\nLessons From Previous Epidemics', ' Recombinant virus-vectored vaccines are an appealing alternative to classical inactivated vaccines because virus vectors enable native expression of influenza antigens, even from virulent influenza viruses, while expressed in the context of the vector that can improve immunogenicity. In addition, a vectored vaccine often enables delivery of the vaccine to sites of inductive immunity such as the respiratory tract enabling protection from influenza virus infection. Moreover, the ability to readily manipulate virus vectors', ' host-oriented antiviral therapies', ' f4f9ea9e0aeb74d3601ee316b84292638c59cc53', ' can interact with pathogens including HIV-1', '', ' Gammaproteobacteria', ' viral antibodies printed directly on 96-well microtiter plates', ' viral infection', ' influenza A virus', '<s>', ' Mean Absolute Percentage Error', ' higher HPAI H5N1 emergence risk', ' increases airway inflammation which aggravates disease symptoms', ' 89% nucleotide identity with bat SARS-like-CoVZXC21 and 82% with that of human SARS-CoV', ' fever disease, headache, rash, nausea, vomiting, myalgia, and arthralgia', '<s>', ' Older males most obviously suffer severe disease', ' New conjugate vaccines', ' 2012', ' 9b0c87f808b1b66f2937d7a7acb524a756b6113b', ' HCoV‐HKU1', ' they cannot maximize realism, generality and precision at the same time', ' six strands', ' high immunogenicity', ' host virulent zoonotic viruses without experiencing disease', ' single stranded RNA', ' CTA1-Conjugated Consensus Matrix Protein-2', ' difficult to answer', ' Hantaviruses', ' Human coronavirus', ' New conjugate vaccines', ' 2015-12-22', ' rhinovirus (23.4%), influenza A virus (21.2%),', ' 2010-04-27', ' eecb946b106a94f26a79a964f0160e8e16f79f42\\n\\nAuthors: le Roux, David M.; Zar, Heather J.\\nDate: 2017-09-21\\nDOI: 10.1007/s00247-017-3827-8\\nLicense: cc-by\\n\\nAbstract: Pneumonia remains the leading cause of death in children outside the neonatal period, despite advances in prevention and management. Over the last 20 years, there has been a substantial decrease in the incidence of childhood pneumonia and pneumonia-associated mortality. New conjugate vaccines against Haemophilus influenzae type b and Streptococcus pneumoniae have contributed to decreases in radiologic, clinical and complicated pneumonia cases and have reduced hospitalization and mortality. The importance of co-infections with multiple pathogens and the predominance of viral-associated disease are emerging. Better access to effective preventative and management strategies is needed in low- and middle-income countries, while new strategies are needed to address the residual burden of disease once these have been implemented.\\n\\nText: Pneumonia has been the leading cause of death in children younger than 5 years for decades. Although there have been substantial decreases in overall child mortality and in pneumonia-specific mortality, pneumonia remains the major single cause of death in children outside the neonatal period, causing approximately 900,000 of the estimated 6.3 million child deaths in 2013 [1]. Substantial advances have occurred in the understanding of risk factors and etiology of pneumonia, in development of standardized case definitions, and in prevention with the production of improved vaccines and in treatment. Such advances have led to changes in the epidemiology, etiology and mortality from childhood pneumonia. However in many areas access to these interventions remains sub-optimal, with large inequities between and within countries and regions. In this paper we review the impact of recent preventative and management advances in pneumonia epidemiology, etiology, radiologic presentation and outcome in children.\\n\\nThe overall burden of childhood pneumonia has been reduced substantially over the last decade, despite an increase in the global childhood population from 605 million in 2000 to 664 million', ' vaccines and therapeutics', ' recent novel findings that elucidate how respiratory viral infections alter the epithelial barrier in the airways, the upper airway microbial environment, epigenetic modifications including miRNA modulation, and other changes in immune responses throughout the upper and lower airways. First, we reviewed the prevalence of different respiratory viral infections in causing exacerbations in chronic airway inflammatory diseases. Subsequently we also summarized how recent models have expanded our appreciation of the mechanisms of viral-induced exacerbations. Further we highlighted the importance of the virome within the airway microbiome environment and its impact on subsequent bacterial infection', ' increases airway inflammation which aggravates disease symptoms', ' WHO European Region', '', '19.2 to 103.5X', ' f6ed1f1e9999e57793addb1c9c54f61c7861a995', ' No credible evidence supporting claims of the laboratory engineering', ' Many questions about its\\norigins, its unusual epidemiologic features, and the basis of\\nits pathogenicity remain unanswered', ' to attempt to elucidate the proximate causes of breeding failure behind the recent decline in productivity in the Spanish Pyrenees', '19', ' 10.1128/mra.01085-19', ' 3.58 from person to person', ' Mother-to-child transmission', ' 1918—1919,\\nwhich caused :50 million deaths worldwide, remains an\\nominous warning to public health. Many questions about its\\norigins, its unusual epidemiologic features, and the basis of\\nits pathogenicity remain unanswered. The public health\\nimplications of the pandemic therefore remain in doubt\\neven as we now grapple with the feared emergence of a\\npandemic caused by H5N1 or other virus. However, new\\ninformation about the 1918 virus is emerging, for example,\\nsequencing of the entire genome from archival autopsy tis-\\nsues. But, the viral genome alone is unlikely to provide\\nanswers to some critical questions. Understanding the\\n1918 pandemic and its implications for future pandemics\\nrequires careful experimentation and in-depth historical\\nanalysis.\\n\\n \\n\\n”Curiouser and curiouser/ ” criedAlice\\nLewis Carroll, Alice’s Adventures in Wonderland, 1865\\n\\nAn estimated one third of the world’s population (or\\nz500 million persons) were infected and had clinical-\\nly apparent illnesses (1,2) during the 191871919 inﬂuenza\\npandemic. The disease was exceptionally severe. Case-\\nfatality rates were >2.5%, compared to <0.1% in other\\ninﬂuenza pandemics (3,4). Total deaths were estimated at\\nz50 million (577) and were arguably as high as 100 mil-\\nlion (7).\\n\\nThe impact of this pandemic was not limited to\\n191871919. All inﬂuenza A pandemics since that time, and\\nindeed almost all cases of inﬂuenza A worldwide (except-\\ning human infections from avian Viruses such as H5N1 and\\nH7N7), have been caused by descendants of the 1918\\nVirus, including “drifted” H1N1 Viruses and reassorted\\nH2N2 and H3N2', ' differences in the p4.7 region between the Irish ECoVs and other ECoVs', '19', ' lack of backup resource. These challenges have caused severe shortage of healthcare workers, medical materials, and beds with isolation. The Spring Festival holiday has greatly aggravated the shortage of human resources and heavy traffic flow due to the vacation of healthy workers and factory workers, which further magnified the risk of transmission. The key point is to discriminate the infectious disease outbreak from regular clustering cases of flu-like diseases at early stage. There is a trade-off between false alarm causing population panic and delayed identification leading to social crisis.\\n\\nEarly identification of 2019-nCoV infection presents a major challenge for the frontline clinicians. Its clinical symptoms largely overlap with those of common acute respiratory illnesses, including fever (98%), cough (76%), and diarrhea (3%), often more severe in older adults with pre-existing chronic comorbidities [1]. Usually, the laboratory abnormalities include lymphocytopenia and hypoxemia [1]. The initial chest radiographs may vary from minimal abnormality to bilateral ground-glass opacity or subsegmental areas of consolidation [1]. In addition, asymptomatic cases and lack of diagnosis kits', ' interferon-induced transmembrane proteins', ' 2010-04-27', ' In this review, we will focus on recent novel findings that elucidate how respiratory viral infections alter the epithelial barrier in the airways, the upper airway microbial environment, epigenetic modifications including miRNA modulation, and other changes in immune responses throughout the upper and lower airways. First, we reviewed the prevalence of different respiratory viral infections in causing exacerbations in chronic airway inflammatory diseases. Subsequently we also summarized how recent models have expanded our appreciation of the mechanisms of viral-induced exacerbations. Further we highlighted the importance of the virome within the airway microbiome environment and its impact on subsequent bacterial infection. This review consolidates the understanding of viral induced exacerbation in chronic airway inflammatory diseases and indicates pathways that may be targeted for more effective management of chronic inflammatory diseases', ' f6fcf1a99cbd073c5821d1c4ffa3f2c6daf8ae29', ' sampling', '<s>', ' The maintenance mechanisms', ' high morbidity and even mortality worldwide', ' Chronic Airway Inflammatory Diseases', ' remains unknown', ' Older males', ' nasal swabs', 'ncbi.nlm', ' lengthy exposure', ' mild and influenza-like illnesses', ' mortality worldwide. The current understanding on viral-induced exacerbations is that viral infection increases airway inflammation which aggravates disease symptoms', '<s>', ' Infections with 2019-nCoV can spread from person to person', ' Isothermal amplification methods, such as reverse-transcription, loop-mediated isothermal amplification (RT-LAMP), exhibit characteristics that are ideal for POC settings, since they are typically quicker, easier to perform, and allow for integration into low-tech, portable heating devices. METHODOLOGY/SIGNIFICANT FINDINGS: In this study, we evaluated the HIV-1 RT-LAMP assay', '<s>', ' strong correlation', '', ' 20 %', ' Estimating the number of infections and the impact of non-\\npharmaceutical interventions', ' improved adaptive neuro-fuzzy inference system', ' 2020', ' prophylactic and therapeutic agent', '', ' substantial decrease in the incidence', ' copyediting, typesetting and review', ' Poor governance, public distrust, and political violence', ' Oude Munnink, Bas B.;', '', '<s>', ' colon', ' landscapes where intensive and extensive forms of poultry production overlap were found at greater risk', ' Réunion Island', 'February 11, 2020', ' Mainly children in developing countries', '', ' 2020', ' virus vectors enable native expression of influenza antigens', ' hantavirus cardiopulmonary syndrome', ' Middle East respiratory syndrome', ' can interact with pathogens including HIV-1', ' modify the subsequent immune response to a vectored antigen', ' native expression of influenza antigens, even from virulent influenza viruses, while expressed in the context of the vector that can improve immunogenicity', ' salp swarm algorithm', ' Influenza A Virus', '<s>', ' 2011-2012', ' fatal', ' phylogenetically independent groups', ' reservoir', ' Influenza A Virus', ' 648', ' Respiratory virus infection', ' severe cases of emerging human adenovirus type 55 (HAdV-55) in immunocompetent adults have been reported sporadically in China. The clinical features and outcomes of the most critically ill patients with severe acute respiratory distress syndrome (ARDS) caused by HAdV-55 requiring invasive mechanical ventilation (IMV) and/or extracorporeal membrane oxygenation (ECMO) are lacking. METHODS: We conducted a prospective, single-center observational study of pneumonia with ARDS in immunocompetent adults admitted to our respiratory ICU. We prospectively collected and analyzed clinical, laboratory, radiological characteristics, sequential tests of viral load in respiratory tract and blood, treatments and outcomes', ' comparing them to licensed vaccines and the hurdles faced for licensure of these next-generation influenza virus vaccines', ' LCMV multiplication', ' Ilorin, Nigeria', ' summer; whereas parainfluenza viruses were identified only during winter', ' by the presence of resistance-associated substitutions (RASs) before and after treatment', ' coronavirus disease', ' full-length cDNA', ' Viruses', ' Seven monoclonal antibodies', ' significantly higher HPAI H5N1 emergence risk', ' 49', ' MTT assay', ' a growing disjunction', ' 1000 bed', ' monoclonal', '<s>', ' CTA1-Conjugated Consensus Matrix Protein-2 (sM2) Induces Broad Protection against Divergent Influenza Subtypes in BALB/c Mice\\n\\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3979752/\\n\\nSHA: efaa556b484fbcd9cc34832ffac53ef3e834e9c0\\n\\nAuthors: Chowdhury, Mohammed Y. E.; Li, Rui; Kim, Jae-Hoon; Park, Min-Eun; Kim, Tae-Hwan; Pathinayake, Prabuddha; Weeratunga, Prasanna; Song, Man Ki; Son, Hwa-Young; Hong, Seung-Pyo; Sung, Moon-Hee; Lee, Jong-Soo; Kim, Chul-Joong\\nDate: 2014-04-08\\nDOI: 10.1371/journal.pone.0094051\\nLicense: cc-by\\n\\nAbstract: To develop a safe and effective mucosal vaccine against pathogenic influenza viruses, we constructed recombinant Lactobacillus casei strains that express conserved matrix protein 2 with (pgsA-CTA1-sM2/L. casei) or without (pgsA-sM2/L. casei) cholera toxin subunit A1 (CTA1) on the surface', ' morbidity and mortality', 'Zixing Huang and Shuang Zhao contributed equally', ' hemorrhagic necrosis and most often fatal outcome', ' 31 kb', ' different cities in China as well as to 24 other countries', ' lower respiratory tract (LRT) disease', '<s>', '<s> What does the confirmatory process aim to  ensure?</s></s> MERS coronavirus: diagnostics, epidemiology and transmission\\n\\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4687373/\\n\\nSHA: f6fcf1a99cbd073c5821d1c4ffa3f2c6daf8ae29\\n\\nAuthors: Mackay, Ian M.; Arden, Katherine E.\\nDate: 2015-12-22\\nDOI: 10.1186/s12985-015-0439-5\\nLicense: cc-by\\n\\nAbstract: The first known cases of Middle East respiratory syndrome (MERS), associated with infection by a novel coronavirus (CoV), occurred in 2012 in Jordan but were reported retrospectively. The case first to be publicly reported was from Jeddah, in the Kingdom of Saudi Arabia (KSA). Since then, MERS-CoV sequences have been found in a bat and in many dromedary camels (DC). MERS-CoV is enzootic in DC across the Arabian Peninsula and in parts of Africa, causing mild upper respiratory tract illness in its camel reservoir and sporadic, but relatively rare human infections. Precisely how virus transmits to humans remains unknown', ' efe13a8d42b60ef9f7387ea539a1b2eeb5f80101\\n\\nAuthors: Hjelle, Brian; Torres-Pérez, Fernando\\nDate: 2010-11-25\\nDOI: 10.3390/v2122559\\nLicense: cc-by\\n\\nAbstract: The continued emergence and re-emergence of pathogens represent an ongoing, sometimes major, threat to populations. Hantaviruses (family Bunyaviridae) and their associated human diseases were considered to be confined to Eurasia, but the occurrence of an outbreak in 1993–94 in the southwestern United States led to a great increase in their study among virologists worldwide. Well over 40 hantaviral genotypes have been described, the large majority since 1993, and nearly half of them pathogenic for humans. Hantaviruses cause persistent infections in their reservoir hosts', ' important interventions for the management', ' Jurkat T-cell nucleolus']\n"
     ]
    }
   ],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Zad13dLZiymy"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "# print(answers)\n",
    "# print(valid_dataset[0])\n",
    "for ref, pred in zip(valid_dataset, answers):\n",
    "  predictions.append(pred)\n",
    "  references.append(answers)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "v8GeAx34iymy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 100.0, 'f1': 96.03960396039604}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "pm8o0R9Biym0"
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0arfVIG4iym1"
   },
   "source": [
    "\n",
    "## Model in action \n",
    "The trained model is available on Huggingface hub if you want to play with it.\n",
    "You can find the model [here](https://huggingface.co/valhalla/longformer-base-4096-finetuned-squadv1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "j85Q67euiym2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerForQuestionAnswering\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "# model = LongformerForQuestionAnswering.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "# print(model)\n",
    "text = \"covid has lead to respiratory issues in humans. The primary target of the SARS-CoV-2 virus, which causes COVID-19, is the respiratory system. The virus primarily affects the respiratory tract, including the nose, throat, and lungs. It gains entry into the body through respiratory droplets when an infected person coughs, sneezes, talks, or exhales. These droplets can be inhaled by nearby individuals, leading to infection. Once inside the body, the virus primarily targets the cells lining the respiratory tract, particularly the cells that line the airways and the alveoli (small air sacs) in the lungs. It attaches to specific receptors on these cells, known as angiotensin-converting enzyme 2 (ACE2) receptors, to gain entry and replicate. COVID-19 can cause a range of respiratory symptoms, including cough, sore throat, shortness of breath, and pneumonia. However, it is important to note that the virus can also affect other organs and systems in the body, such as the cardiovascular system, gastrointestinal system, kidneys, liver, and neurological system. Severe cases of COVID-19 can lead to complications and multiorgan involvement, which can result in a more severe illness. It's worth mentioning that the impact of COVID-19 can vary from person to person, and some individuals may experience more severe respiratory symptoms than others. Additionally, emerging research continues to shed light on the diverse effects of the virus on different body systems.\"\n",
    "question = \"what part of the body does corona viruus affect the most?\"\n",
    "encoding = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Op-c7N_Aiym3"
   },
   "source": [
    "default is local attention everywhere\n",
    "the forward method will automatically set global attention on question tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7JF7LHkOiym3"
   },
   "outputs": [],
   "source": [
    "attention_mask = encoding[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uH8Y21cGiym4"
   },
   "outputs": [],
   "source": [
    "start_scores, end_scores = model(input_ids, attention_mask=attention_mask)\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hQIGkDggiym4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " respiratory tract\n"
     ]
    }
   ],
   "source": [
    "answer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\n",
    "answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "print(answer)\n",
    "# output => democratized NLP"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
